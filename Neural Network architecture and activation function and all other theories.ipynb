{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e7d2d4e",
   "metadata": {},
   "source": [
    "# `Neural Network architecture and activation function`\n",
    "### Note: ML and DL are completely different\n",
    "### Too much is bias is also not good for model\n",
    "### In the entire journey of NN or DL our objective is to keep our bias as small as possible\n",
    "![](https://miro.medium.com/max/659/1*2doUN-Ce1kH38NvfHNicxg.png)\n",
    "![](https://preview.redd.it/pjj39fhl1pc61.png?width=640&format=png&auto=webp&s=37fd1d99b4bf9aa2ff64975ba64daa52a3a141f4)\n",
    "\n",
    "# `Activation Functions`:\n",
    "\n",
    "## `Decision Making Activation Function`:\n",
    "1. **Sigmoid Activation Function**: gives us a range of prediction between 0 to 1.\n",
    "- Disadvantage: Computation cost is high\n",
    "- Advantage: Helps to classify between two classes very efficiently.\n",
    "![](https://machinelearningmastery.com/wp-content/uploads/2021/08/sigmoid.png)\n",
    "2. **Hard Sigmoid Activation Function**: Repharsing the entire formula to replace exp to reduce compuation cost. Used in IoT devices\n",
    "- **f(x) = max(0,min(1,(x+1)/2))**\n",
    "3. **Sigmoid Weighted Linear Units**: Used only for **reinforcement learning**, research based.\n",
    "- **Automatic Learning**\n",
    "- **f(x) = x * sigmoid(x)** => research paper\n",
    "- **f(x) = zk * w * (zk), zk=w\\*i + b, w=weights** => rishab sir\n",
    "\n",
    "### [silu research paper](https://paperswithcode.com/method/silu)\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_4.17.41_PM.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "4. **Linear Activation Function**: don't apply any activation function and use rmse your model is a linear regression model.\n",
    "### [Activation Functions and their Derivatives â€“ A Quick & Complete Guide](https://www.analyticsvidhya.com/blog/2021/04/activation-functions-and-their-derivatives-a-quick-complete-guide/)\n",
    "![](https://editor.analyticsvidhya.com/uploads/94131Screenshot%20(43).png)\n",
    "![](https://dustinstansbury.github.io/theclevermachine/assets/images/a-gentle-introduction-to-neural-networks/common_activation_functions.png)\n",
    "\n",
    "5. **Hyperbolic tangent Activation Function / tanh Activation Function**: gives us a range between  (-1 to +1), -1 => negative 0 => neutral 1 => positive. Tells us sense of classification. Mainly used in **sentiment analysis**, NLP related algo.\n",
    "- Advantage: It works better with different layers of neurons. It gives us strong point of polarization. Widely used in NLP and NLU related stuff. like sentiment analysis , topic modelling.\n",
    "- Disadvantage: Heavy in computation.\n",
    "![](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_4.23.22_PM_dcuMBJl.png)\n",
    "![](https://ars.els-cdn.com/content/image/1-s2.0-S016971611830021X-f09-17-9780444640420.jpg)\n",
    "\n",
    "6. **Hard Tanh**: Computation becomes cheap. **Used in IoT device**\n",
    "- f(x):\n",
    "    1. -1, if x<-1\n",
    "    2. x, if -1<= x <=1\n",
    "    3. +1, x>1\n",
    "    \n",
    "7. **Softmax Activation Function**: Can classify 100 and 1000 of classses.\n",
    "\n",
    "#### `Output` is being derived from the `Decision Making Activation Function`\n",
    "\n",
    "## `Feature Selector Activation Function`:\n",
    "#### `Feature Selector Activation Function`:\n",
    "- **selecting feature, shape, color, activating those nearby pixels, activating those nearby cells, selecting the particular tokens, feature of a text or image is selected**.\n",
    "1. **Rectified Linear Units (ReLu)**: \n",
    "- Advantage: Computationally cheaper.\n",
    "- Disadvantage: It easily overfits and gradient die(you do not learn much features). Do not work well with negative value.\n",
    "- **get dead neurons problem**\n",
    "- Conclusion: **ReLu works much better in computer vision than NLP stuffs**.\n",
    "- **f(x) = max(0,x)**\n",
    "- In NLP we use **Leaky ReLu**\n",
    "\n",
    "![](https://www.researchgate.net/profile/Junxi-Feng/publication/335845675/figure/fig3/AS:804124836765699@1568729709680/Commonly-used-activation-functions-a-Sigmoid-b-Tanh-c-ReLU-and-d-LReLU.ppm)\n",
    "\n",
    "\n",
    "2. **Leaky ReLu**: **prevent us from dead neurons problem**\n",
    "- f(x) = alpha\\*x + x:\n",
    "    - **x if x>=0**\n",
    "    - **alpha\\*x if x<0**\n",
    "    - **alpha is constant, any high dim negative value will be converted to small negative value. alpha=0.01/can change according to need**.\n",
    "    - **Researches found 0.01 useful**.\n",
    "3. **P-Relu / Parametric ReLu**: Learn while it trains, alpha changes based on learning, have capability to learn while it trains(learning of alpha).\n",
    "![](https://imgs.developpaper.com/imgs/2813863991-5e720eab30026_articlex.png)\n",
    "- Here alpha keeps on changing based on learning factor, like weights are changed by backpropogation algo, alpha changes in each an every step based on learning factor. learns while it trains, works better then ReLU and Leaky ReLU. Here alpha is dependent on data, not universal like in Leaky ReLu.\n",
    "- At the time backpropogation changes the weights, it will also change alpha value. It changes itself based on the data.\n",
    "4. **Randomized ReLu**: **alpha is defined based upon uniform distribution function**. Used in **medical datasets**.\n",
    "- **blood sugar level**: below 120 it's bad and above 150 it's bad, limiting to** a certain range, reject everything beyond range.\n",
    "![](https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-3-030-48453-8_1/MediaObjects/485563_1_En_1_Fig5_HTML.png)\n",
    "\n",
    "5. **Exponential Linear Units (ELU)**: It is costly in computation. Neurons are not dead here.\n",
    "- **Change very near negtive values to positive values**.\n",
    "![](https://tungmphung.com/wp-content/uploads/2020/02/elu_function.png)\n",
    "![](https://miro.medium.com/max/1400/1*RD0lIYqB5L2LrI2VTIZqGw.png)\n",
    "6. **Swish Activation Function (Designed by Google)**: Designed for application of their algo. basically used for NLP stuff and network greater than 40 layers. It is used as, shall we should select the feature or not. \n",
    "- **f(x) = x * sigmoid (x)**. \n",
    "- **2 level of feature selector**.\n",
    "![](https://i.ytimg.com/vi/JewUzs5XugE/maxresdefault.jpg)\n",
    "7. **Maxout Activation Function**: \n",
    "- **calculating weight.T * x1 + b,weight.T * x2 + b,weight.T * x3 + b,weight.T * x4 + b.....weight.T * xn + b.....and calculating max out out it**.\n",
    "- There is no zero, no dying of neurons, no overfitting, no data shortage. Just taking the maximum value.\n",
    "![](https://miro.medium.com/max/1200/1*Ae28NCJH6ZYooc64jIPYQw.jpeg)\n",
    "8. **Sinc Activation Function**: Developed by NASA for robots.\n",
    "- **y = Sin(x)/x where x=!0**\n",
    "![](https://i2.wp.com/sefiks.com/wp-content/uploads/2018/01/sinc-function.png?resize=543%2C311&ssl=1)\n",
    "9. **Softplus Activation Function**: It is **not a decision making nor feature selector** it is used for making **learning smooth**. To make the difference smooth during gradient descdent.\n",
    "- while learning it gives some sort of predictions, predictions it tries to generate results, that results is getting compared with actuals and that **difference** is again gets adjusted in the model and model again go back and learn the parameters....and repeat. To smooth ou this **diiference** we use softplus.\n",
    "- Using lesser iteration will achieve higher sccuracy.\n",
    "- Mathematical Equation : **f(x) = log(exp^(x)+1)**\n",
    "![](https://cdn-images-1.medium.com/max/800/1*p_hyqAtyI8pbt2kEl6siOQ.png)\n",
    "10. **Mish Activation Function**:\n",
    "- **f(x) = x(tanh(softplus(x)))**, range is -1 to +1. **3 level of feature selector**.\n",
    "- It is using softplus AF.\n",
    "![](https://www.researchgate.net/profile/Zhihao-Cao-3/publication/349558817/figure/fig5/AS:1002554452373513@1616039016278/Leaky-ReLU-and-Mish-activation-functions.jpg)\n",
    "#### `If your project needs 3 level of feature selection, go with MISH`\n",
    "#### `If your project needs 2 level of feature selection, go with SWISH`\n",
    "\n",
    "![](https://i.redd.it/sylrjb6e8gk61.jpg)\n",
    "**ReLu, Leaky ReLu, Sigmoid, Hard Sigmoid, ELU, Softmax, tanh** : if we consider only these we can crack any interview.\n",
    "\n",
    "## `Backpropogation Algorithm`:\n",
    "![](http://hmkcode.github.io/images/ai/bp_update_formula.png)\n",
    "![](https://media.cheggcdn.com/study/225/2256b52e-cd65-4c6e-a47b-a316d9f3ce5b/image.png)\n",
    "![](https://miro.medium.com/max/1298/1*1HLVrhU-Flfs9av1S2iBIA.png)\n",
    "- Hidden layer it is always 2/3 of input layer of the features by some research.\n",
    "- **Weight_new = Weight_old - learning_rate * (derivative of loss/derivative of old weight)**\n",
    "- **Chanin rule = y = f(x), x = f(t), dy/dt = dy/dx * dx/dt**\n",
    "- **dl/dwn = p1 + p2 + p3.....pn (p=path)**\n",
    "- **When we will stop updating weights?? That is called Gradient Descent**\n",
    "- **You can always say that no of hidden layer can be 2/3 * input layer**\n",
    "- **By using hidden layers we simplify image into a very small level**\n",
    "- **Later on we use these updated weights to predict objects**.\n",
    "![](https://www.researchgate.net/publication/342922622/figure/fig1/AS:913194927288326@1594734045877/An-example-of-a-forward-propagation-neural-network-The-first-layer-The-input-is.png)\n",
    "\n",
    "### `In hidden layers feature selection takes place`\n",
    "\n",
    "## `Learning Rate`: \n",
    "### `How fast we are learning the weights`\n",
    "![](https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png)\n",
    "### `Follow heuristic approach to find best parameters`\n",
    "- **n in gradient descent => Learning rate + momentum**\n",
    "![](https://miro.medium.com/max/491/1*n4ftRAKEJu8-gLB3pzY0DA.png)\n",
    "![](https://miro.medium.com/max/566/1*2wULsk4M4HG12bZ5cB-bPA.png)\n",
    "- Here **alpha => Learning rate + momentum**\n",
    "- Learning rate: how fast we are reaching to global minima.\n",
    "- Momentum: how smooth we are reaching to global minima.\n",
    "- Now this LR is now further included into multiple different mathematical models:\n",
    "1. **Exponential weighted average**: it has two components\n",
    "- beta\\*a1 + (i-beta)\\*a2\n",
    "- a1: different instances of LR \n",
    "- a2: different instances of momentum\n",
    "- beta: used to adjust between LR and momentum\n",
    "- both LR and momentum are inversely propotion with the help of beta\n",
    "- There are some more modification came into the picure, some of them discovered a new kinds of GD optimizers:\n",
    "2. **Adagrad optimizer**:\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/10/21.3.png)\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/10/21.4.png)\n",
    "- Here learning rate is not fixed change based upon loss.\n",
    "- For every epoch LR is updated.\n",
    "3. **Root Mean Square Prop optimizer(RMSProp)**:\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/10/21.5.png)\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/10/21.6.png)\n",
    "- There can be a instance where previous LR could be better then present, we can revert back to previous point.\n",
    "- Here beta helps us to give preference to previous or cureent LR.\n",
    "- Mostly used in Regression.\n",
    "4. **Adam optimizer**:\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/10/21.7.png)\n",
    "![](https://www.sravikiran.com/GSOC18//images/update_eqn.jpg)\n",
    "- Includes both the property of Adagrad+RMSProp\n",
    "- **In most of our problems we use Adam optimizer**.\n",
    "\n",
    "## `Gradient Descent`:\n",
    "![](http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization_files/ball.png)\n",
    "![](![](https://miro.medium.com/max/1400/1*2aS_8T7-f5gkoE-3gA9JlA.png))\n",
    "![](https://i.ytimg.com/vi/b4Vyma9wPHo/maxresdefault.jpg)\n",
    "#### `Softplus` activation function helps us to remove local minima condition and make global minima better, it smooths the curve.\n",
    "- Model irregular hyperparameters are responsible for model stucking in local minima, hence model do not gives good accuracy.\n",
    "- If data is not well set or it have issues it is also responsible for model stucking in local minima, , hence model do not gives good accuracy.\n",
    "- **Gradient Descent** is known as the **optimizer** to Backpropogation.\n",
    "- There are scenerios where you cannot reach global minima.\n",
    "1. **Vanishing gradient** \n",
    "2. **Exploding gradient**\n",
    "- **Momentum** makes the curve smoother which helps to reduce the number of epochs.\n",
    "- **Learning rate** helps us to fast-forward our training.\n",
    "- **Backpropogation updates the weight and GD tells till when we need to update weights**.\n",
    "\n",
    "### Curve between `learning accuracy` and `prediction accuracy` is always good method for analysis for overfitting.\n",
    "\n",
    "## `Momentum`:\n",
    "![](https://qph.fs.quoracdn.net/main-qimg-cb86c89148422852a2eda8ec42fe4700-pjlq)\n",
    "- It smoothens the learning curve which actually helps us to reduce epochs.\n",
    "![](https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png)\n",
    "### 1. `Vanishing gradient`:\n",
    "- Your weights remains unchanged/saturated.\n",
    "- It happen when you have very dense network/very large network with many hidden layers.\n",
    "- Takes huge time to reach global minima.\n",
    "- **reasons**: \n",
    "1. no change in weight \n",
    "2. If there is a **lot of bias in the network**\n",
    "- **Accuracy will not change much**\n",
    "\n",
    "### 2. `Exploding gradient`:\n",
    "- Your model is trying to learn a feature, but the feature weights values are so high that whenever we are going to adjust the weights, the loss value is so high that it start exploding here and there. It never reaches global minima.\n",
    "- **To prevent Exploding Gradient** -\n",
    "- **Standarizing your data points**: If data is having negative values.\n",
    "- **Normalizing your data points**: If data is having positive values.\n",
    "- Accuracy will explode here and there.\n",
    "\n",
    "## `Different Kinds of  Gradient Descent/Optimizer`:\n",
    "![](https://miro.medium.com/max/1400/1*PV-fcUsNlD9EgTIc61h-Ig.png)\n",
    "![](https://miro.medium.com/max/1838/1*5mHkZw3FpuR2hBNFlRxZ-A.png)\n",
    "### 1. `GD with entire data`:\n",
    "- Also called **batch GD**, neural network tries to learn the features from all the data. **Time consuming process**. Learning capability is high.\n",
    "- **No overfitting**\n",
    "- eg. for batchsize=10,for 10000 records, one batch will have 1000 records. In one epochs 10 batches will be called one by one and model will learn. At the end of epoch model with predict. then the loss will be calculated and taken by backpropogation for weight updation to minimize the loss.\n",
    "- Aggregate of all the 10 batches is basically loss of entire data.\n",
    "\n",
    "### `Callbacks`:\n",
    "- **Will monitor at what point you are getting the minimum loss. It will stop the epochs when it found the min loss as global minima**.\n",
    "\n",
    "### 2. `GD with Random data/Stochastic GD`:\n",
    "- **Takes samples of data points randomly**.\n",
    "- **Does not go with all the records**.\n",
    "- Cheaper in computation.\n",
    "- **Need to keep high number of epochs**.\n",
    "- **Makes the model more biased**.\n",
    "\n",
    "### 3. `Mini Batch GD `:\n",
    "- **Overfitting problem goes away, mostly used**.\n",
    "- **Mini Batch GD = Batch GD + Stochastic GD**\n",
    "- Cheaper in computation\n",
    "- **DataPoints in epoch 1 will not match with epoch 2**.\n",
    "\n",
    "## [A Comprehensive Guide on Deep Learning Optimizers](https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/)\n",
    "\n",
    "### 4.`Adagrad`:\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/10/21.3.png)\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/10/21.4.png)\n",
    "### 5. `RMSProp`:\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/10/21.5.png)\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/10/21.6.png)\n",
    "\n",
    "### 6. `ADAM`:\n",
    "- Adam optimizitation is a technique in which you take exponential weighted average of your GD.\n",
    "- You reach global minima faster with the help of Adam.\n",
    "- Adam = Adagrad + RMSprop\n",
    "- **Most famous and mostly used optimizer**\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/10/21.7.png)\n",
    "![](https://www.sravikiran.com/GSOC18//images/update_eqn.jpg)\n",
    "\n",
    "## `Softmax Activation Function`:\n",
    "- **Can classify upto thousands of classes**.\n",
    "- In 90% DL algo in classification problem we use Softmax.\n",
    "![](https://miro.medium.com/max/1400/1*ReYpdIZ3ZSAPb2W8cJpkBg.jpeg)\n",
    "![](https://miro.medium.com/max/1838/1*eqQuFgXPUP5L6J_vVH19wg.png)\n",
    "\n",
    "\n",
    "## `Cross-entropy`:\n",
    "![](https://pmirla.github.io/assets/entropy/formula2.png)\n",
    "![](https://miro.medium.com/max/919/1*ETtY7KCrzAlOmLeyDWE4Xg.png)\n",
    "- **How well you are actually learning the data points**.\n",
    "- **How well the classification have been made this can be solved by cross-entropy**.\n",
    "- **Cross Entropy = - (label(dog) * log(dog) + label(cat) * log(cat) + label(rabbit) * log(rabbit) + label(squirrel) * log(squirrel))**\n",
    "- In wrong classification CE of actual vs CE of predicted will differ.\n",
    "- Softmax and CE are inter-linked.\n",
    "\n",
    "### `It have different types`:\n",
    "1. **Categorical CE**: **You deal with multiple classes** \n",
    "2. **Binary CE**: **You deal with two classes**\n",
    "\n",
    "## `Dropout`:\n",
    "\n",
    "### `Normal Dropout`:\n",
    "- **Overfit comes with lack of data, lack of variety**.\n",
    "- A mechanism to reduce overfitting and bias.\n",
    "- In a given neural network some of the neurons gets biased, it basically trim down some nodes and without that nodes it tries to train the model.\n",
    "- nodes are neurons.\n",
    "\n",
    "### `Special Dropout`: *****\n",
    "- **It do no drop neurons, rather you drop the features**.\n",
    "- **Don't apply to much regularization else you will land in underfit**.\n",
    "\n",
    "#### `Too much dropout will make your model underfit`\n",
    "\n",
    "## `Regularization`:\n",
    "![](https://miro.medium.com/max/1358/1*MKvhDD5DzckYkRFB-H71SA.png)\n",
    "![](https://miro.medium.com/max/560/1*zYfwoRcih4jzyDP3j3aVmQ.png)\n",
    "### [Regularization in Deep Learning](https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/)\n",
    "\n",
    "## `Batch Normalization`:\n",
    "![](http://csmoon-ml.com/wp-content/uploads/2019/04/1WRio7MD4JDeLww-CyrxEbg.png)\n",
    "### [Introduction to Batch Normalization](https://www.analyticsvidhya.com/blog/2021/03/introduction-to-batch-normalization/)\n",
    "\n",
    "## `Connect all these dots together`:\n",
    "### `Entire Pipeline`:\n",
    "**Data => Preprocessing => train_test split => pass it to Neural Networks => O/P => pass to applications**\n",
    "\n",
    "## `Weight Initialization according to researchers`:\n",
    "#### `Key Points`:\n",
    "1. weights should be small\n",
    "2. weights should not be same\n",
    "3. weights should have good variance\n",
    "\n",
    "## ` Weight Initialization VariousTechniques`:\n",
    "### 1. `Uniform Distribution W.I technique`\n",
    "### 2. `Xavier / Gorat Distribution W.I technique (works nicely with sigmoid A.F)`\n",
    "    a) Xavier / Gorat Normal\n",
    "    b) Xavier / Gorat Unifrom(works nicely with sigmoid A.F)\n",
    "### 3. `'He' init W.I technique (works nicely with ReLU A.F)`\n",
    "    a) 'He' uniform\n",
    "    b) 'He' normal\n",
    "\n",
    "## `Important Points`:\n",
    "\n",
    "1. [Amazon Go Store](https://www.youtube.com/watch?v=NrmMk1Myrxc&ab_channel=amazon)\n",
    "2. Tensorflow is designed by google.\n",
    "3. Tensorflow has it's own datastructure.\n",
    "4. To make tensorflow more simple we have keras.\n",
    "5. In 2020 Tensorflow aquired keras.\n",
    "6. From Tf 2.0, keras is included in tensorflow.\n",
    "7. Pytorch is made by **Facebook**. good for computer vision projects.\n",
    "8. Start with tf and keras later you can also learn pytorch.\n",
    "9. Keras is on top of tensorflow, tensorflow is on top of your shell, shell is top of your kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b88886",
   "metadata": {},
   "source": [
    "# `END ---------------------------------------`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
