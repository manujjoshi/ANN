{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e7d2d4e",
   "metadata": {},
   "source": [
    "# `Neural Network architecture and activation function`\n",
    "### Note: ML and DL are completely different\n",
    "### Too much is bias is also not good for model\n",
    "### In the entire journey of NN or DL our objective is to keep our bias as small as possible\n",
    "\n",
    "# `Activation Functions`:\n",
    "## `Decision Making Activation Function`:\n",
    "1. **Sigmoid Activation Function**: gives us a range of prediction between 0 to 1.\n",
    "- Disadvantage: Computation cost is high\n",
    "- Advantage: Helps to classify between two classes very efficiently.\n",
    "![](https://machinelearningmastery.com/wp-content/uploads/2021/08/sigmoid.png)\n",
    "2. **Hard Sigmoid Activation Function**: Repharsing the entire formula to replace exp to reduce compuation cost. Used in IoT devices\n",
    "- **f(x) = max(0,min(1,(x+1)/2))**\n",
    "3. **Sigmoid Weighted Linear Units**: Used only for **reinforcement learning**, research based.\n",
    "- **Automatic Learning**\n",
    "- **f(x) = x * sigmoid(x)** => research paper\n",
    "- **f(x) = zk * w * (zk), zk=w\\*i + b, w=weights** => rishab sir\n",
    "### [silu research paper](https://paperswithcode.com/method/silu)\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_4.17.41_PM.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "4. **Linear Activation Function**: **don't apply any activation function and use rmse your model is a linear regression model**.\n",
    "### [Activation Functions and their Derivatives â€“ A Quick & Complete Guide](https://www.analyticsvidhya.com/blog/2021/04/activation-functions-and-their-derivatives-a-quick-complete-guide/)\n",
    "![](https://editor.analyticsvidhya.com/uploads/94131Screenshot%20(43).png)\n",
    "![](https://dustinstansbury.github.io/theclevermachine/assets/images/a-gentle-introduction-to-neural-networks/common_activation_functions.png)\n",
    "5. **Hyperbolic tangent Activation Function / tanh Activation Function**: gives us a range between  (-1 to +1), -1 => negative 0 => neutral 1 => positive. Tells us sense of classification. Mainly used in **sentiment analysis**, NLP related algo.\n",
    "- Advantage: It works better with different layers of neurons. It gives us strong point of polarization. Widely used in NLP and NLU related stuff. like sentiment analysis , topic modelling.\n",
    "- Disadvantage: Heavy in computation.\n",
    "![](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_4.23.22_PM_dcuMBJl.png)\n",
    "![](https://ars.els-cdn.com/content/image/1-s2.0-S016971611830021X-f09-17-9780444640420.jpg)\n",
    "6. **Hard Tanh**: Computation becomes cheap. **Used in IoT device**\n",
    "\n",
    "#### Output is being derived from the Decision Making Activation Function\n",
    "\n",
    "## `Feature Selector Activation Function`:\n",
    "1. **Rectified Linear Units (ReLu)**: \n",
    "- Advantage: Computationally cheaper.\n",
    "- Disadvantage: It easily overfits and gradient die(you do not learn much features). Do not work well with negative value.\n",
    "- Conclusion: ReLu works much better in computer vision than NLP.\n",
    "- In NLP we use Leaky ReLu\n",
    "- **get dead neurons problem**\n",
    "2. **Leaky ReLu**: **prevent us from dead neurons problem**\n",
    "3. **P-Relu / Parametric ReLu**: Learn while it trains, alpha changes based on learning, have capability to learn while it trains(learning of alpha).\n",
    "4. **Randomized ReLu**: alpha is defined based upon uniform distribution function. Used in medical datasets.\n",
    "5. **Exponential Linear Units (ELU)**: It is costly in computation. Neurons are not dead here.\n",
    "6. **Swish Activation Function (Designed by Google)**: Designed for application of their algo. basically used for NLP stuff and network greater than 40 layers. It is used as, shall we should select the feature or not. **y = x * sigmoid (x)**. **2 level of feature selector**.\n",
    "7. **Maxout Activation Function**: \n",
    "8. **Sinc Activation Function**: Developed by NASA for robots.\n",
    "9. **Softplus Activation Finction**: It is not a decision making nor feature selector it is used for making learning smooth. \n",
    "10. **Mish Activation Function**: **f(x) = x(tanh(softplus(x)))**, range is -1 to +1. **3 level of feature selector**.\n",
    "11. **Softmax Activation Function**: Can classify 100 and 1000 of classses.\n",
    "\n",
    "**ReLu, Leaky ReLu, Sigmoid, Hard Sigmoid, ELU, Softmax, tanh** : if we consider only these we can crack any interview.\n",
    "\n",
    "## `Backpropogation Algorithm`:\n",
    "- Hidden layer it is always 2/3 of input layer of the features by some research.\n",
    "- **Weight_new = Weight_old - learning_rate * (derivative of loss/derivative of old weight)**\n",
    "- **Chanin rule = y = f(x), x = f(t), dy/dt = dy/dx * dx/dt**\n",
    "- **dl/dwn = p1 + p2 + p3.....pn (p=path)**\n",
    "- **When we will stop updating weights?? That is called Gradient Descent\n",
    "\n",
    "- `In hidden layers feature selection takes place`\n",
    "\n",
    "## `Learning Rate`: \n",
    "\n",
    "\n",
    "\n",
    "## `Gradient Descent`:\n",
    "- **Softplus activation function** helps us to remove local minima condition and make global minima better, it smooths the curve.\n",
    "- Model irregular hyperparameters are responsible for model stucking in local minima, hence model do not gives good accuracy.\n",
    "- If data is not well set or it have issues it is also responsible for model stucking in local minima, , hence model do not gives good accuracy.\n",
    "- Gradient Descent is known as the optimizer to Backpropogation.\n",
    "- There are scenerios where you cannot reach global minima.\n",
    "1. **Vanishing gradient** \n",
    "2. **Exploding gradient**\n",
    "- **Momentum** makes the curve smoother which helps to reduce the number of epechs.\n",
    "- **Learning rate** helps us to fastforward our training.\n",
    "\n",
    "\n",
    "### 1. `Vanishing gradient`:\n",
    "- Your weights remains unchanged/saturated.\n",
    "- It happen when you have very dense network/very large network with many hidden layers.\n",
    "- Takes huge time to reach global minima.\n",
    "- **reason**: 1. no changed in weight 2. If there is a lot of bias in the network\n",
    "- Accuracy will not change much.\n",
    "\n",
    "### 2. `Exploding gradient`:\n",
    "- Your model is trying to learn a feature, but the feature weights values are so high that whenever we are going to adjust the weights, the loss value is so high that it start exploding here and there. It never reaches global minima.\n",
    "- To prevent Exploding Gradient -\n",
    "- **Standarizing your data points**: If data is having negative values.\n",
    "- **Normalizing your data points**: If data is having positive values.\n",
    "- Accuracy will explode here and there.\n",
    "\n",
    "## `Different Kinds of  Gradient Descent`:\n",
    "### 1. `GD with entire data`: \n",
    "- Also called **batch GD**, neural network tries to learn the features from all the data. Time consuming process. Learning capability is high.\n",
    "- No overfitting\n",
    "\n",
    "#### `Callbacks`:\n",
    "- Will monitor at what point you are getting the minimum loss. It will stop the epochs when it found the min loss as global minima.\n",
    "\n",
    "### 2. `GD with Random data/Stochastic GD`:\n",
    "- Takes samples of data points randomly.\n",
    "- Does not go with all the records.\n",
    "- Cheaper in computation.\n",
    "- Need to keep high number of epochs.\n",
    "- Makes the model more biased.\n",
    "\n",
    "### 3. `Mini Batch GD `:\n",
    "- Overfitting problem goes away, mostly used.\n",
    "- **Mini Batch GD = Batch GD + Stochastic GD**\n",
    "- Cheaper in computation\n",
    "\n",
    "## `Softmax Activation Function`:\n",
    "- Can classify upto thousands of classes.\n",
    "- In 90% DL algo in classification problem we use Softmax.\n",
    "\n",
    "\n",
    "## `Cross-entropy`: \n",
    "- How well the classification have been made this can be solved by cross-entropy.\n",
    "- **CE = - (label(dog) * log(dog) + label(cat) * log(cat) + label(rabbit) * log(rabbit) + label(squirrel) * log(squirrel))**\n",
    "\n",
    "### `It have different types`:\n",
    "1. **Categorical CE**: You deal with multiple classes \n",
    "2. **Binary CE**: You deal with two classes\n",
    "\n",
    "## `Normal Dropout`:\n",
    "- Overfit comes with lack of data, lack of variety.\n",
    "- A mechanism to reduce overfitting and bias.\n",
    "- In a given neural network some of the neurons gets biased, it basically trim down some nodes and without that nodes it tries to train the model.\n",
    "- nodes are neurons.\n",
    "\n",
    "## `Special Dropout`:\n",
    "- It do no drop neurons, rather you drop the features.\n",
    "- Don't apply to much regularization else you will land in underfit.\n",
    "\n",
    "## `Connect these dots`:\n",
    "### `Entire Pipeline`:\n",
    "- Data => Preprocessing => train_test split => Neural Networks => O/P => pass to applications \n",
    "\n",
    "## `Weight Initialization according to researchers`:\n",
    "#### `Key Points`:\n",
    "1. weights should be small\n",
    "2. weights should not be same\n",
    "3. weights should have good variance\n",
    "\n",
    "### ` W.I. VariousTechniques`:\n",
    "1. Uniform Distribution W.I technique: \n",
    "2. Xavier / Gorat Distribution W.I technique (works nicely with sigmoid A.F)\n",
    "    a) Xavier / Gorat Normal\n",
    "    b) Xavier / Gorat Unifrom(works nicely with sigmoid A.F)\n",
    "3. He init W.I technique (works nicely with ReLU A.F)\n",
    "    a) He uniform\n",
    "    b) He normal\n",
    "\n",
    "## `Important Points`:\n",
    "\n",
    "1. [Amazon Go Store](https://www.youtube.com/watch?v=NrmMk1Myrxc&ab_channel=amazon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b88886",
   "metadata": {},
   "source": [
    "## `END ---------------------------------------`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
